\chapter{State of Art}
\label{chapter:SA}

This dissertation suggests the development of a conversational query builder that functions as a chat-based interface within the EHDEN project ecosystem. This interface aims to help researchers redefine their studies effectively. The conversational assistant should return a ranked database list to help discover the best databases for specific research and also collect additional information to define an observational study. To accomplish these goals, {\ir} is a crucial field to study because it can retrieve the most appropriate databases for a researcher's query. In addition, it is crucial to explore the recent advancement of algorithms, such as {\llm}, known for their language generation ability. Understanding the current state of conversational user assistants or chatbots is also essential.


\section{Information Retrieval}

In computing and information science, {\ir} involves retrieving information from collections of unstructured data, such as documents. According to~\citet{p_m_efficient_2021}, an {\ir} system requires input user queries and uses them to retrieve information from its collections, aligning with the users' needs. This process efficiently filters and narrows down the vast amount of available unstructured data. It prevents users from being overwhelmed by the sheer volume of data and aiding them in quickly accessing relevant and specific content.

In conformity with~\citet{hambarde_information_2023}, conventional text retrieval systems were predominant in the initial stages of the {\ir} field. These systems mainly depended on matching terms between queries and documents. However, these conventional {\ir} systems have limitations, including issues like polysemy, synonymy, and linguistic gaps, which may restrict their effectiveness~\cite{hambarde_information_2023}.

With the advancement of technology, deep learning techniques emerged, improving conventional text retrieval systems and overcoming the constraints associated with term-based retrieval methods. For this reason, the performance of these systems increased significantly, resulting in a more accurate and streamlined retrieval of information for end-users~\cite{mitra_introduction_nodate}.

Deep learning methods have advanced, with the emergence of neural network architectures, transfer learning, and pre-training techniques. These approaches have advanced the representation of textual data and bolstered the {\ir} system's comprehension of natural language queries~\cite{mitra_introduction_nodate}.

More recently, transformer architectures with attention mechanisms have been implemented in {\ir} systems to enable more effective handling of complex queries and documents. Moreover, incorporating pre-trained language models like {\bert}~\cite{devlin_bert_2018} and {\gpt}-2 has proven to enhance the performance of {\ir} systems. These methods offer an advanced understanding and processing of context, capturing the relationships and nuances within the natural language query and the documents.

This field has many applications in the real world, such as search engines like Google, digital libraries, and e-commerce platforms. In compliance with~\citet{p_m_efficient_2021}, {\ir} generally functions across three main scales: i) searching the web, ii) retrieving personal information, and iii) conducting searches for enterprises, institutions, and domain-specific contexts. This section describes the {\ir} field, more specifically, traditional methods, neural {\ir} systems and how {\ir} can be joined with natural laguage.


\subsection{Traditional methods}

Some successful classical methods are {\tfidf} and {\bm}.

\subsubsection{TF-IDF}

To better understand the {\tfidf} method, first, it is necessary to understand the concepts of {\tf} and {\idf}.

Starting with {\tfidf} method, it is composed of the {\tf} and the {\idf}, two essential concepts to understand this classical {\ir} method.~\citet{liang_research_2022} explained these concepts in their work. It is reasonable to assume that a document containing a query term more frequently is more relevant to that query. Therefore, it should be assigned a higher relevance and/or score. {\tf} is the number of term occurrences in a document. However, to evaluate the relevancy of a query, each term is regarded with equal importance.~\citet{manning_introduction_2009} provided the following example to better explain {\tf}: the automotive industry is expected to include the term ``auto'' in nearly every document. The most frequent terms are not always the most important. Equation~\ref{eqn:tf} shows how the {\tf} is calculated.

\begin{equation}
    \textup{tf}_{t,d} = \frac{\text{Term \textit{t} frequency in document \textit{d}}}{\text{Total words in document \textit{d}}}.
    \label{eqn:tf}
\end{equation}

In order to reduce the weight of terms that appear frequently, the {\idf} helps to prioritize some terms that are sporadic and possibly more informative and relevant~\cite{liang_research_2022} . The {\idf} calculates the rarity of a term across a set of documents. This measure is calculated as the logarithm of the division of the total number of documents by the number of documents in the collection containing the term, as the Equation~\ref{eqn:idf} describes. Aligning with the previous example of the automotive industry, the generic term ``auto'' will have a low {\idf} value. However, the term ``telematics'' will have a high IDF value, underlining the unique and informative significance of the term for distinguishing documents.

\begin{equation}
    \textup{idf}_{t} = \log\bigg(\frac{\text{Total documents}}{\text{Documents with term \textit{t}}}\bigg).
    \label{eqn:idf}
\end{equation}

The TF-IDF method combines TF and IDF to assign a weight to each term in a document, highlighted by as~\citet{manning_introduction_2009} and~\citet{lan_research_2022}. Equation~\ref{eqn:tfidf} shows that the weight is calculated by multiplying the TF and IDF values. So, it is possible to highlight terms that are both important within a specific document and relatively uncommon in the document collection.

\begin{equation}
    \textup{tf-idf}_{t,d} = \textup{tf}_{t,d} \times \textup{idf}_{t}.
    \label{eqn:tfidf}
\end{equation}

{\tfidf} does not consider the semantic information of words, which limits its ability to accurately reflect the similarity between texts~\cite{lan_research_2022}. In summary, this {\ir} method evaluates the importance of a term within a document relative to its occurrence across a collection of documents.


\subsubsection{BM25}
\label{bm25}

{\bm} is a ranking algorithm for {\ir} systems, especially in the context of search engines.~\citet{hambarde_information_2023} stated that {\bm} and other initial retrievers are employed for their effectiveness in recalling pertinent documents from an extensive pool. The core components of {\bm} include {\tf}, {\idf}, {\dl}, and tuning parameters. Equation~\ref{eqn:bm25} gives the formula to calculate the {\bm} score.

\begin{equation}
score(D,Q) = \sum_{i}^{n} IDF(q_{i}) \ast \frac{f(q_{i},D) \ast (k1 + 1)}{f(q_{i},D) + k1 \ast (1 - b + b \ast \frac{fieldLen}{avgFieldLen})}.
\label{eqn:bm25}
\end{equation}

As~\citet{phd_understanding_2023} explained, the {\bm} equation is composed of the \textit{ith} query term (q) and the respective {\idf} and {\tf} values. Also, include a division between {\dl} (fieldLen) and the average document length (avgFieldLen). This ratio evaluates how much the length of the document field deviates from the average length.~\citet{noauthor_practical_2018} explained intuitively: \textit{a document tends to receive a lower score when it contains more terms, especially those that do not match the query}. The value \textit{b} is a fine-tuning parameter, and it is responsible for length normalization. When \textit{b} is larger, the ratio has a more significant effect on the overall score. Finally, the \textit{k1} value means term frequency saturation. It is a fine-tuning parameter that prevents the term frequency component of BM25 from having an unlimited impact on the document score.

This algorithm can be simple and effective in {\ir} tasks, mainly search tasks. Also, it can handle large collections. For these reasons, it is widely used and called a classic. However, {\bm} can not perform a semantic analysis of the query and the documents.

% Another limitation is the ignorance of other crucial factors to get a better search beyond the factors relative to the {\tf} and {\dl}.


%% ponte de tradicionais para os mais recentes
Traditional {\ir} methods like {\tfidf} and {\bm} have been effective in matching queries to documents based on keyword frequencies and document lengths. However, the evolution of {\ai} has led to the development of neural {\ir} systems to adress some limitations of these classical methods.



\subsection{Neural information retrieval systems}

Neural {\ir} systems represent a significant advancement in the field of {\ir}. Conforming to~\citet{mitra_introduction_nodate}, these systems use neural network models and deep learning techniques. This approach can understand and interpret the semantic content of queries and documents. Neural {\ir} systems can be broadly categorized into two types: representation-based and interaction-based models.

Conforming to~\citet{chen_integrating_2023}, the representation-based model focuses on creating representations of both queries and documents. They employ techniques to convert text into high-dimensional vector spaces. Semantically similar terms and phrases are represented by vectors that are close to each other, capturing the underlying meaning and relationships of words beyond their surface-level appearances. This approach employs architectures like {\rnn} and allows the system to understand the content of documents and queries on a deeper level, as~\citet{liu_deep_2022} mentioned.

The interaction-based model examines how words in a query relate to words in a document, capturing complex patterns and dependencies between them~\cite{chen_integrating_2023}. They often use attention mechanisms, as seen in transformer-based models like {\bert}, to dynamically weigh the importance of different parts of the text based on their relevance to the query. However, only the interaction-based model will be explored.

There are different approaches to implementing an interaction-based model. One of these approaches involves a retrieval stage and a ranker stage~\cite{mitra_introduction_nodate}.~\ref{fig_ir_system} shows an overview of interaction-based {\ir} systems, highlighting the two main stages.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{figs/chapter2/IR_system.png}
    \centering
    \caption[Overview of an interaction-based Information Retrieval model]{Overview of an interaction-based Information Retrieval model: Retrieval and Ranker.}
    \label{fig_ir_system}
\end{figure}

After analyzing the query, the retrieval stage selects an initial set of documents that are potentially pertinent to the query, as shown in Figure~\ref{fig_ir_system}. Subsequently, the relevance of these documents undergoes reassessment through the similarity scores. This is followed by the ranking stage, in which the primary objective is to adjust the order of the initially retrieved documents based on their relevance scores using a neural reranking, like {\bert}~\cite{chen_integrating_2023}. This phase prioritizes the enhancement of result effectiveness rather than efficiency. In the end, it returns a rank of documents as close as possible to the user's query criteria. 


\subsection{Question answering}

{\nlp} is the basis for building a {\qa} system. The {\nlp} can be divided into two major components: {\nlu} and {\nlg}, according to~\citet{ayanouz_smart_2020}.

The {\nlu} component is focused on enabling machines to understand and interpret human language in a meaningful way. {\nlu} involves processing and analyzing natural language data to comprehend its meaning, context, sentiment, and intent. 

In agreement with~\citet{ngai_intelligent_2021}, the user queries can be processed by semantic analysis, pragmatic analysis, and syntactic analysis.~\citet{ayanouz_smart_2020} explained these steps and added two more necessary steps to make it easier to understand: a lexical analysis and discourse integration. The process of query analysis by {\nlu} involves the following steps:

\begin{itemize}
    \item \textbf{Lexical Analysis}: This step involves analyzing and identifying the structure of words. It breaks down the text into chapters, sentences, phrases, and words.~\citet{chizhik_challenges_2020} defined lexical analysis as the pre-processing of the text following the steps: tokenization, removal of special characters, links, and punctuation, and removal of stop-words.

    \item \textbf{Syntactic Analysis}: The syntactic analyzer parses the grammar and arrangement of words, making the relationships between different words more explicit. Essentially, it rejects sentences with incorrect structures. This analysis can be seen as the process of normalizing tokens.

    \item \textbf{Semantic Analysis}: This step ensures the text is meaningful and interprets its correct meaning by mapping syntactic constructions. It ensures that only semantically valid content is retained. The recognition of entities is part of this analysis.

    \item \textbf{Pragmatic Analysis and Discourse Integration}: This step analyzes the overall context to derive the conclusive interpretation of the actual message in the text. It considers factors like the true meaning of a phrase or sentence based on the broader context.
\end{itemize}

The other component is {\nlg}. Language generation is responsible for crafting coherent and linguistically accurate responses. It transforms data into natural language text, enabling effective communication and information dissemination~\cite{ngai_intelligent_2021}. 

Backtracking, {\qa} is a subfield of {\ir} and {\nlp}. According to~\citet{zhong_building_2020}, {\qa} focuses on providing a single and specific answer to a question posed in natural language. Unlike {\ir}, which aims to return a broad range of relevant information or documents in response to a query, {\qa} seeks to pinpoint and provide one precise answer.

The traditional approach to question analysis and answering often involves mapping questions into predefined templates, such as ``What-type'' and ``How-type''. While widely used by existing online question-answering search engines, this template-based approach faces limitations in handling multiple questions~\cite{zhong_building_2020}. 

With the advancement of technology, another approach emerged: deep learning-based question-answering~\cite{chizhik_challenges_2020}. In contrast with the traditional approach, this approach employs deep learning techniques, like {\rnn}, to offer automatic representation and analysis of questions. These neural models, trained through end-to-end approaches, excel in extracting and understanding complex characteristics in textual documents.

Recently, deep learning approaches with attention mechanisms and transfer learning have enhanced the flexibility of representation in text classification and named entity recognition.~\citet{zhong_building_2020} highlights {\bert} that has emerged as a powerful model, using contextualized representations for transfer learning. {\bert}-based models showcase performance in question-answering tasks, even in domains like medicine.



\section{Large Language Models}
\label{llm}

Before {\llm}, there were only simple {\lm}, a subfield of {\nlp} and {\ai}, that have been called foundation models. A {\lm} is a statistical model used to predict the next word in a sequence of words. It calculates the probability of a given word occurring in a sequence, helping to determine which words are likely to appear next in a given context~\cite{chang_language_2023}.

Most of these predictive models were based on probabilities and Markov assumptions, also known as {\slm}. This was heavily dependent on feature engineering. Afterward, as deep learning gained prominence, an architecture designed to learn data features automatically. In other words, neural networks for {\nlp} emerged to enhance {\lm}'s capabilities. Integrating feature learning and model training, {\nlm} established a comprehensive neural network framework applicable to diverse {\nlp} tasks~\cite{liu_prompting_nodate}.

Most recently, the launch of the transformer Block Architecture by~\citet{vaswani_attention_2023} revolutionized this field. These deep-learning architectures led to the development of pre-trained models not explicitly designed for a particular task, including {\bert} and {\gpt}, collectively known as {\plm}. {\plm} have shown significant performance enhancements across various {\nlp} tasks.

The scale of the parameters of these models has been involved, and the paradigm of ``Pre-train, Prompt, Predict'' like~\citet{liu_prompting_nodate} described, gained widespread acceptance. In terms of interaction with {\lm}, the prompts became crucial. Researchers name these {\plm} with hundreds of billions of parameters as {\llm}. Prompts effectively allow {\llm} to deal with a large number of complex and diverse tasks without a lot of effort.


\subsection{Definition}

{\llm} is designed to comprehend and generate text that is coherent and contextually relevant, engaging in human language interactions. Essentially, these advanced AI systems mimic human speech. These advanced {\ai} systems  have a notable ability in natural language tasks, such as text generation and translation, question-answering, decision-making, summarization, and sentiment analysis~\cite{zhao_survey_2023}.

These models can process and predict patterns with accuracy.~\citet{hadi_LLM_2023} combine sophisticated {\slm} and deep learning techniques to train, analyze, and understand huge volumes of data, learning the patterns and relationships among the data. For this reason, according to~\citet{naveed_comprehensive_2023}, when provided with task descriptions and examples through prompts, {\llm} can produce textual responses to task queries. 

~\citet{liu_prompting_nodate} noted that the release of the first version of ChatGPT garnered significant social attention, and research into {\llm} triggered more interest. This has led to the development of noteworthy products like PaLM~\cite{anil_palm_2023}, {\gpt}-2, {\gpt}-3, and, most recently, {\gpt}-4~\cite{openai_gpt-4_2023}, and LLaMA and LLaMa-2~\cite{touvron_llama_2023}.


\subsection{Architecture overview}

The development and advancement of {\llm} is thankful for the introduction of transformers by~\citet{vaswani_attention_2023} in 2017. Most {\llm} are built on the transformer model, which is based on a multihead self-attention mechanism and feedforward layers. This new technology enables parallelization and efficient handling of long-range dependencies, according to~\citet{hadi_LLM_2023}, and led to the development of models that have achieved enormous results, such as {\gpt} by OpenAI and {\bert} by Google. 


\subsubsection{Transformer architeture}

The architecture of this revolutionized model is shown in Figure~\ref{fig_trans_arch}. The transformer architecture uses stacked self-attention and point-wise fully connected layers for both the encoder and decoder. These components are illustrated in the left and right halves of Figure~\ref{fig_trans_arch}, respectively.

The innovation of this model is due to the multihead self-attention mechanism, which is one of its key components~\cite{vaswani_attention_2023, hadi_LLM_2023}. It allows the model to weigh the importance of different words in a sequence when processing each word. This mechanism enables the model to focus on relevant information, capturing dependencies regardless of word order. However, the key advantage of this multihead self-attention mechanism is its highly parallelization~\cite{vaswani_attention_2023}. This characteristic enables the transformer model to be easily distributed and trained on a large scale using GPUs. The ability to parallelize computations means that transformers can handle larger datasets and more complex tasks, unlike previous architectures like {\rnn}, where sequential processing of data was required.

Since the model does not have recurrence and convolution to understand the order of the input sequence, another component, position encoding, provides some information about the position of the tokens in the sequence. This is crucial for capturing sequential information in the data. 

\begin{figure}[ht]
    \includegraphics[width=0.5\textwidth]{figs/chapter2/transformer.png}
    \centering
    \caption[The transformer block architecture]{The transformer block architecture, from~\citet{vaswani_attention_2023}.}
    \label{fig_trans_arch}
\end{figure}


\subsubsection{Pre-training process}

Learning the patterns and relationships among the data starts with the pre-training process. In compliance with~\citet{min_recent_pretrained}, the {\llm} needs to access a vast volume of textual data from multiple sources. The goal of this phase is to predict the succeeding word in a sentence based on the context given by the previous words through unsupervised learning. 

According to~\citet{hadi_LLM_2023}, preparing and preprocessing the data before the training stage is necessary to achieve this. The fisrt step is demand quality filtering from the training corpus. It is vital to remove unwanted, repetitive, duplicated, superfluous, and potentially harmful content from the massive text data. Next, it is necessary to pay attention to privacy. The data could have sensitive or personal information, so it is vital to address privacy concerns by removing this information from the pre-training corpus.

An important step, the tokenization, follows this~\cite{hadi_LLM_2023}. This step aims to divide the unprocessed text into sequences of individual tokens, which are subsequently input into {\llm}. Moreover, it is vital in mitigating the computational load and enhancing efficiency during the pre-training phase. Figure~\ref{fig_tokenization} visually presents the tokenization process~\cite{noauthor_openai_nodate} carried out and explained by OpenAI. In the example of the Figure~\ref{fig_tokenization}, the text inserted has 58 tokens. The colors are meaningless and used to illustrate each token. After the pre-training process, the {\llm} goes through an optimization phase.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{figs/chapter2/tokenization.png}
    \centering
    \caption[Tokenization example]{Tokenization by {\gpt}-4 example~\cite{noauthor_openai_nodate}: the colors are used to illustrate each token.}
    \label{fig_tokenization}
\end{figure}



\subsection{Optimization techniques}

There are some techniques to optimize the tasks and the accuracy of the {\llm}. Fine-tuning and prompt engineering are techniques commonly used to enhance the performance of models for specific tasks. These techniques play a crucial role in customizing models and improving their precision for specific applications.


\subsubsection{Fine-tuning}

During pre-training, models are generally trained with the objective of next token prediction, learning the nuances of language structure and semantics. According to~\citet{kamnis_generative_2023} and~\citet{hadi_LLM_2023}, the fine-tuning phase involves adapting a pre-trained model to specific tasks and aligning it with human preferences, improving the performance on particular domains.

In this stage, the model is presented with labeled data to produce more contextually accurate responses for the specific task. Fine-tuning enables the {\llm} to specialize in diverse applications, ranging from language translation and question-answering to text generation. 

Some approaches could be applied to fine-tune the model.~\citet{naveed_comprehensive_2023} distinguishes some of them, such as parameter-efficient tuning. As {\llm} typically requires a lot of computational resources, like memory and computing, the parameter-efficient tuning approach is helpful because it allows the model to train by updating fewer parameters, adding new ones, or selecting existing ones. Inside this approach, there are also some different methods. The commonly used indicated by~\citet{naveed_comprehensive_2023} are prompt tuning, prefix tuning, and adapter tuning.

The prompt tuning method integrates trainable tokens, named soft prompts, to the beginning or within the input of a {\llm}, and only these tokens are adjusted during training to adapt the model for a specific task. This method keeps the rest of the model unchanged, ensuring the core knowledge and capabilities of the model are preserved while it learns to handle new types of requests or information.

In prefix tuning, a sequence of trainable tokens is introduced to transformer layers, with only the prefix parameters undergoing fine-tuning, while the remaining model parameters remain unchanged. These added prefixes function as virtual tokens, allowing input sequence tokens to attend to them during processing.

Meanwhile, in adapter tuning, small modules called adapters are added inside each layer of the transformer. These adapters can be trained to adapt the model for specific tasks. The fine-tuning process works by slightly altering the model's internal features, allowing it to learn task-specific patterns without changing the entire model. {\lora} is one technique that implements Adapter Tuning, introduced by~\citet{hu_lora_2021}. Instead of adding new layers like traditional adapters, {\lora} learns low-rank matrices that are used to update the weights of the existing layers, maintaining the original weights of the model. This approach allows for efficient fine-tuning of the model on specific tasks while maintaining the model's original performance and avoiding significant increases in computational costs.


\subsubsection{Prompt engineering}

With the emergence of {\llm}, other research fields were born. Prompt Engineering is one of these cases and has been widely applied. In compliance with~\citet{mesko_prompt_2023} and~\citet{ma_beyond_2023}, this emerging field involves designing, refining, and implementing prompts or instructions to direct the generated output of {\llm}, aiding in diverse tasks. {\llm} can follow specific directions provided by users in natural language after being tuned with instructions.

There are some techniques of prompt engineering, such as {\chain} and {\react}. {\chain} is a popular problem-solving approach for prompt engineering that aims to break complex tasks into multiple and simpler subtasks and solve them.~\citet{wei_chain--thought_2023} explained that this method involves explicitly modeling the reasoning processes that lead to a final answer, rather than directly generating an answer. This explanation of reasoning often leads to more accurate results. The example of the Figure~\ref{fig_cot} contrasts standard prompting with {\chain} prompting. In the standard prompting, the {\llm} prompt is a math problem with an example that provides directly the respective answer. This approach leads to an incorrect response. Conversely, the {\chain} prompting includes the same math problem but with an example that provides a step-by-step breakdown of the calculations. The detailing of reasoning makes the difference and leads to the correct answer.

{\react} is a prompt technique introduced by~\citet{yao_react_2023}. The idea behind this is to simultaneously include both reasoning and action within a single prompt. To solve a complex task, {\react} consists of three tasks for every subtask: 1) \textbf{Reason} involves analyzing the current situation and determining the necessary steps; then, 2) \textbf{Action} entails executing a task based on the reasoning. 3) \textbf{Observation} then refers to examining the outcomes following the action.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{figs/chapter2/CoT.png}
    \centering
    \caption[Example of Chain-of-Thought prompting]{Example of Chain-of-Thought prompting. The reasoning processes are highlighted in yellow. Adapted from~\citet{wei_chain--thought_2023}.}
    \label{fig_cot}
\end{figure}

% ~\citet{mesko_prompt_2023} presents a set of recommendations for creating more effective prompts for {\llm}. The key points include:

% \begin{itemize}
%     \item The prompt must be as precise as possible.
%     \item Provide context and setting of the prompt to the {\llm}.
%     \item Describe the prompt's goal at the outset.
%     \item Assign a specific role to the {\llm} to provide more context, for example, ``You are a math teacher and explain the natural numbers.''.
%     \item Continuous {\llm} prompt refinement.
%     \item Regularly testing prompts in real-world situations
% \end{itemize}


\subsection{Comparison between foundation Large Language Models}

The best way to compare {\llm} is to evaluate the model's performance.~\citet{hadi_LLM_2023} identified five factors to make this comparison: the size of the training corpus, the quality of the training corpus, the number of parameters, the complexity of the model, and some test tasks.

The primary foundation models of {\llm} are {\gpt}-4 by OpenAI, LLaMA 2 by Meta, PaLM 2 by Google, and Falcon by {\tii}. These {\llm} are provided by big companies and have outstanding progress in the evolution of this area. These models gave rise to many others.

LLama 2~\cite{touvron_llama_2023} is an open source {\llm} by Meta. LLaMa 2 was trained on 40\% more data than LLaMa, the model from which it came, and has double the context length. The model size of LLaMa 2 is 7 billion, 13 billion, or 70 billion parameters. With 4096 context length and trained on 2 trillion pretraining tokens, this {\llm} is commonly fine-tuned for chat use cases. Many other models, like Alpaca, Vicuna, and Llama-2-chat, came from LLaMa and deserve further analysis. It is accessible for both research and commercial purposes

The recent {\gpt} model from OpenAI, {\gpt}-4~\cite{openai_gpt-4_2023}, is a closed source {\llm}. Trained on a meticulously curated dataset from various textual sources, including books, articles, and websites, {\gpt}-4 exhibits remarkable performance with text and image inputs. It is the {\llm} behind ChatGPT. It has 32 000 context length. OpenAI has chosen to provide limited technical details about the training methodology used for this advanced model, including specific information on parameter counts.

The Google generative chatbot, Bard, uses as {\llm} the PaLM 2 model~\cite{anil_palm_2023} developed by Google. It emerged from PaLM with 540 billion parameters. PaLM 2 is a closed source {\llm} that opted to disclose limited technical specifics, as the {\gpt}-4 by OpenAI. 

The Falcon {\llm} is an open-source model with impressive performance and scalability~\cite{almazrouei_falcon_2023}. There are three variations of the model size: 7 billion, 40 billion, and the most recent, 180 billion of parameters. The Falcon 180B is equipped with an impressive 180 billion parameters and trained on 3.5 trillion tokens. It is accessible for both research and commercial purposes.

The Mistral\cite{jiang_mistral_2023} was introduced by Mistral AI. It is an open source {\llm} with 7 billion of parameters. This model has 8 192 context length and trainde on 128K tokens. Mistral 7B originated two other models: Mixtral 8x7B~\cite{jiang_mixtral_2024} and Mixtral 8x22B. The table~\ref{table:comparison} summarizes the important aspects of comparison between this foundation {\llm}.

\begin{table}[ht]
    \def\arraystretch{1.5}
    \scalebox{0.82}{
        \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Provider} & \textbf{\begin{tabular}[c]{@{}c@{}}Model size\\ (Parameters)\end{tabular}} & \textbf{Context Length} & \textbf{Tokens} & \textbf{Fine-tuneability} & \textbf{Open-source} \\ \hline
        GPT-4          & OpenAI            & -                                                                          & -                       & -               & No                        & No                   \\ \hline
        LLaMa 2        & Meta              & 7B, 13B, 70B                                                               & 4096                    & 2T              & Yes                       & Yes                  \\ \hline
        PaLM 2         & Google            & -                                                                          & -                       & -               & No                        & No                   \\ \hline
        Falcon         & {\tii}            & 7B, 40B, 180B                                                              & 2048                    & 3.5T            & Yes                       & Yes                  \\ \hline
        Mistral        & Mistral AI        & 7B                                                                         & 8192                    & 128K            & Yes                       & Yes                  \\ \hline
        \end{tabular}
    }
    \caption[Comparison of foundation Large Language Models]{Comparison of foundation Large Language Models.}
    \label{table:comparison}
\end{table}

The table comprises some important aspects, such as if the model is opened or closed source. The source availability of the model is an important aspect in order to choose the {\llm} for this project. Since the proposed chatbot is intended to help support medical studies, \textit{i.e.}, the chatbot may process sensitive information, it is necessary a private installation of the {\llm}. The {\gpt} and PaLM models can not be used due to this restriction.



\subsection{Limitations}

According to~\citet{liu_prompting_nodate}, the abilities of {\llm}, mainly in-context learning, reasoning for complex content, and creative capacity, proved the versatility of these advanced {\lm}. However, {\llm} has some limitations.~\citet{hadi_LLM_2023} address some of them, and the most important ones are biased responses, hallucination, explainability, and cyber-attacks. 

We already know that {\llm} are pre-trained with extensive training data. But suppose that data contains some biased information related to factors such as gender, socioeconomic status, and/or race. In that case, this may result in analyzes and recommendations that are discriminatory or inaccurate across diverse domains. The problem of bias applies not only to training data but also to user interaction bias, algorithmic bias, and contextual bias. The user interaction bias means that, as user prompts shape responses, and if users consistently ask biased or prejudiced questions, the model may acquire and reinforce these biases in its replies.

A severe limitation that is an active area of research is hallucination.~\citet{church_emerging_2023} characterized {\llm} hallucinations as when the model attempts to fill gaps in knowledge or context, relying on learned patterns during training. Such occurrences can result in inaccurate or misleading responses, detrimental to the user and the model's reliability.

The way the {\llm} makes decisions is unknown. Comprehending the decision-making process of a complex model with billions of parameters is difficult. The explainability of these models is a big limitation~\cite{hadi_LLM_2023}. Sometimes, it is necessary to decipher the factors that influenced an {\llm}'s decision and this limitation poses difficulties in offering a clear and concise explanation. In vital sectors like healthcare, where decisions carry substantial consequences, ensuring transparency and the capability to elucidate the model's predictions is essential.

Another limitation is the cyber-attacks. A {\llm} can suffer some prompt injections from a malicious user to extract sensitive information from the model, according to~\citet{kshetri_cybercrime_2023}. This is called the Jail Break attack~\cite{hadi_LLM_2023}. Another attack is Data Poisoning Attacks, which consist of data poisoning strategies to manipulate the model's output.

Furthermore,~\citet{liu_prompting_nodate}  identified another limitation: the outdated nature of the training data. LLMs cannot access real-time information, meaning the generated responses may not be the most current.



\section{Conversational user assistants}

Conversational user assistants, also known as chatbots, chatterbots, or virtual assistants, have become a vital aspect of the digital landscape. These tools are generally dialogue systems that understand, interpret, and generate human language, enabling them to communicate with users to dissolve their questions~\cite{borah_survey_2019}.

Chatbots are increasingly being used in various contexts due to their many benefits. These aspects that make companies bet on the use of chatbots are the continuous availability to support and assist the customer, ensuring more consistent support; the cost-efficiency by reducing the human customer support; the time-saving both for the organization and for customers due to the immediate responses to the user queries; the ease and intuitiveness of this systems; and, improve service with every interaction~\cite{misischia_chatbots_2022}. Because of this, the utility of the chatbots as tools is increasing as the technology advances. The rise of conversational user assistants is underpinned by a convergence of technologies, specifically by {\llm}.


\subsection{Overview of conversational user assistants}

~\citet{nuruzzaman_survey_2018} defined the differences between chatbots with opened or closed domains. In an open-domain environment, conversations can go in any direction without a predefined goal or intention. Conversely, in closed-domain environments, the conversation is centered on a particular topic. A closed-domain chatbot is designed with a clear objective.

~\citet{peng_survey_2019} distinguish three main types of chatbots based on their response generation: rule-based, retrieval-based and generative-based chatbots. A \textbf{rule-based chatbot} examines fundamental features of the user's input statement and generates a response based on a predefined set of manually crafted templates. This type is more applicable in a closed-domain conversation. ELIZA, introduced by~\citet{weizenbaum_elizacomputer_1966}, was the first chatbot that applied this primitive technique.

A \textbf{retrieval-based chatbot} chooses a response from an extensive precompiled dataset. It selects the most promising reply from the top-k ranked candidates. Thus, they refrain from producing new text. It has limited flexibility regarding closed-domain and in terms of errors~\cite{agarwal_review_2020}.

A \textbf{generative-based chatbot} generates a text sequence as a response rather than choosing it from a predefined set of candidates. These chatbots are very flexible and can handle open domains because they are implemented with deep learning techniques. The interactions will be more identical to those of humans, as it implements a self-learning method from a large quantity of interaction data~\cite{peng_survey_2019, agarwal_review_2020}. However, this could be complex and costly to implement.


\subsection{Generative-based chatbot}

Generative-based conversational user assistants are chatbots that use generative models to generate natural language responses. These chatbots use sophisticated deep-learning techniques, such as {\llm}. These have the ability to understand and generate human-like text in context. ChatGPT is an example of this type of chatbot.

Although an {\llm} can generate text from a query, it is not prepared to be applied to a chatbot. There are some techniques for improving and optimizing the model to behave like a chatbot, such as {\rlhf}. In addition, some techniques aim to combat some of the limitations of chatbots, such as hallucination, by increasing their knowledge, such as {\rag}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% commented %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
       

% There are some methods to improve the results of a {\llm} used on chatbot: fine-tuning the {\llm}, {\rag}, and Prompt Engineering.

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{|cc|cc|}
%     \hline
%     \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{}}}                                                                                 & \multicolumn{2}{c|}{\textbf{Model Adaptation Required}}                                                                   \\ \cline{3-4} 
%     \multicolumn{2}{|c|}{}                                                                                                           & \multicolumn{1}{c|}{Low}                                                                             & High               \\ \hline
%     \multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}External \\ Knowledge \\ Required\end{tabular}}}} & Low  & \multicolumn{1}{c|}{Prompt Engineering}                                                              & Fine-tuning        \\ \cline{2-4} 
%     \multicolumn{1}{|c|}{}                                                                                                    & High & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Retrieval-Augmented \\ Generation (RAG)\end{tabular}} & All of the methods \\ \hline
%     \end{tabular}
%     \caption{Optimization methods of generative-based chatbots.}
%     \label{table:optimization}
% \end{table}

% The table~\ref{table:optimization} shows how these methods influence model adaptation and external knowledge. Mixing all methods requires a high model adaptation and external knowledge, but provides better results. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% commented %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Reinforcement Learning from Human Feedback}

According to~\citet{li_human-centered_2019}, {\rlhf} is a popular approach in which an agent learns how to perform a task based on evaluative feedback provided by a human observer. It is a topic that has been explored in the field of conversational {\ai}. This technique is a transformative technique that combines reinforcement learning and supervised learning to refine {\llm} for chatbot applications.~\citet{tran_enhancing_2023} stated that {\rlhf} aims to align chatbot responses to human preferences, improving chatbots' performance and making them more human-like.

The process encompasses several crucial stages, in conformity with~\citet{axelsson_modeling_2022}. Initially, the {\llm} is pre-trained on a large dataset of text, which allows it to learn a wide range of language patterns and knowledge. After pre-training, the model undergoes a phase of supervised fine-tuning. In this phase, the {\llm} is trained on a dataset of conversational examples that are specifically curated to reflect the desired outputs for the chatbot. 

Afterwards, humans provide feedback on the model's outputs. This feedback is crucial because it is used to build and train a reward model. The reward model learns to predict the quality of the model's responses based on the human-provided feedback~\cite{axelsson_modeling_2022}.

The {\llm} is further fine-tuned using reinforcement learning, where it learns to generate responses that maximize the predicted reward, using the reward model created in the previous phase. This stage enables the model to optimize its responses through the reward model, which is based on human feedback.

The process often involves several iterations of feedback and fine-tuning to continually improve the chatbot's performance. This can resolve errors, improving the refining the conversational style.



\subsubsection{Retrieval-Augmented Generation}
\label{rag_section}

{\rag} is a subfield of {\nlp} and {\ai}. This approach was introduced by~\citet{lewis_retrieval-augmented_2020} in 2020 and combines retrieval-based and generative models to enhance content generation and information retrieval processes. For a clearer comprehension of this method,~\citet{gao_retrieval-augmented_2023} made a survey into {\rag} systems and distinguish the parametric knowledge from non-parametric knowledge. 

Traditionally, {\llm} can adapt their knowledge and responses to a specific domain by fine-tuning models with parameters. This is parametric knowledge because the {\llm} knowledge is provided through the model's training data. However, entirely parameterized {\llm} have limitations, including data not currently updated and hallucinations. The non-parametric knowledge, provided by external information sources, emerged to solve these limitations. This non-parametric knowledge approach is known as {\rag}. 

{\rag} involves retrieving pertinent information from external knowledge bases, providing the {\llm} with up-to-date and domain-specific context to enhance response accuracy and relevance, thereby reducing hallucinations. According to~\citet{lewis_retrieval-augmented_2020}, this process aims to retrieve relevant information from a vast corpus of documents and incorporate it into the prompt to improve the quality of predictions.

Using the Figure~\ref{fig_rag} as an example,~\citet{gao_retrieval-augmented_2023} explain simply the workflow of a {\rag}. The first step aims to retrieve information from an external data source, such as a vector database (step 1). This step uses {\ir} models, such as {\bm}, to retrieve relevant information based on the query. The second step aims to add the information retrieved as context to the {\llm} prompt (step 2). In the last step, using the prompt with context, the {\llm} generates a response to the query, based on the external information retrieved (step 3).


%% FIXAR ESTA IMAGEM
% image reference: https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2
\begin{figure}[ht]
    \includegraphics[width=\textwidth]{figs/chapter2/rag_workflow.png}
    \centering
    \caption[Retrieval-Augmented Generation workflow]{Retrieval-Augmented Generation workflow: Retrieve, Augment and Generate.}
    \label{fig_rag}
\end{figure}


% escrever mais? vantagens,  RAG vs Fine-tuning, tipos de RAG


\section{Interactive query builder}

A query builder is a user interface tool for dynamically searching and filtering database objects, constructing a query according to user preferences, as the work of~\citet{mussa_forestqb_2022} shows. This query could be in different formats, such as SQL and JSON. This tool lets users construct queries visually, eliminating manual research or coding. 

This section is particularly significant as there is limited documentation on conversational query builders. Therefore, it documents the general workings of query builders and delves into a detailed explanation of the functioning of the ATLAS cohort definition.

\subsection{General query builders}

% explicar como funcionam de forma geral

Users interact with the query builder through a user-friendly interface. Figure~\ref{fig_query_builder} shows a query builder interface from jQuery QueryBuilder~\cite{noauthor_jquery_nodate}.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{figs/chapter2/querybuilder.png}
    \centering
    \caption[Simple query builder example]{Simple query builder from jQuery QueryBuilder~\cite{noauthor_jquery_nodate}.}
    \label{fig_query_builder}
\end{figure}

Users can add rules and conditions/groups with some clicks. Each rule typically consists of a field, an operator, and a value. The conditions/groups could be an AND or an OR. Using figure~\ref{fig_query_builder} as an example, there is a group with two elements joined with a condition AND: a rule and another group. This other group is composed of two rules joined with a condition OR.

As users build their queries, the query builder internally represents the conditions in a structured format, often a structured JSON of rules and groups, that reflects the logical structure of the query~\cite{noauthor_jquery_nodate}. In summary, a query builder simplifies creating complex queries by providing a visual and interactive interface, making it more accessible~\cite{noauthor_introducing_2021}.

% There are several advantages of its use~\cite{noauthor_introducing_2021}: offers a user-friendly interface with menus, operators, and suggestions to facilitate the creation of accurate queries; users, often without direct permissions to modify the data source, can leverage the query builder to transform datasets without making changes to the underlying database; and, the generated queries are easily modifiable, allowing for flexibility in adjustments or repetitions.

\subsection{OHDSI cohort creator}
\label{atlas}

% explicar como funciona o processo
{\ohdsi} provides diverse open-source tools to support various use cases on observational patient-level data. One of these software tools is ATLAS\footnote{\url{https://github.com/OHDSI/Atlas}}. ATLAS is a freely accessible, open-source, web-based software developed by the {\ohdsi} community. It aids in helping researchers conduct scientific analysis on standardized observational data converted to the {\omop}.

Using healthcare claims data, researchers can define cohorts by categorizing groups of people according to their exposure to a medication or their diagnosis of a certain health condition. ATLAS offers the functionality to search medical concepts, enabling the identification of cases with particular conditions or drug exposures. Moreover, it allows for the examination of patient profiles within a given cohort, providing a way to visualize the healthcare records of specific subjects.

There are some different definitions of cohort, but, in the {\ohdsi} research, a cohort is a query that defines a set of persons who meet certain inclusion criteria over a specified duration~\cite{informatics_chapter_nodate}. Cohorts serve as fundamental units for addressing research questions. A key characteristic of these cohorts is their independent definition. The distinct structure facilitates their reuse across different research contexts.


\subsubsection{Cohort definition}

In ATLAS, the process of defining a cohort is composed of 3 stages~\cite{informatics_chapter_nodate}: cohort entry events, inclusion criteria, and cohort exit.The creation of a cohort starts with cohort entry events, defining the initial event criteria. This involves the primary identification of the population of interest, which might include users of a certain drug, individuals with a specific diagnosis, or a combination of factors. The concept set needs to be specified in the cohort entry events. It is a collection of standardized medical concepts used to define clinical elements like diseases, drugs, or procedures. Additional initial event criteria can also be added to refine the population further, such as the event occurring within a certain time frame.

After defining the initial event, the next step is to establish inclusion criteria. These criteria are based on a combination of domain-specific attributes to further refine and specify the cohort population, ensuring that it aligns closely with the research objectives. The inclusion criteria can be based on a range of factors such as age limits, the presence of certain symptoms, or a specified duration of medication use.

Finally, defining the cohort exit criteria is crucial for determining when individuals no longer belong to the cohort. This stage is important for studies where the duration of membership in the cohort is relevant to the research question.

After defining the cohort, it is possible to generate an SQL code with the query to get the list of individuals who meet the criteria. ATLAS also facilitates the reuse of cohort definitions across different studies by allowing users to export and import cohort definitions in JSON format. This enhances the efficiency and reproducibility of research within the OHDSI network. A cohort definition can be seen as a query builder, a little different when compared to other general query builders. Figure~\ref{fig_atlas} shows an example of a cohort definition in the ATLAS tool.

% meter uma imagem da interface ?

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{figs/chapter2/atlas.png}
    \centering
    \caption[Example of a cohort definition in ATLAS]{Example of a cohort definition in ATLAS.}
    \label{fig_atlas}
\end{figure}


\section{Summary}

The dissertation involves the development of a conversational query builder. This system should be a generative-based chatbot with a closed domain. Closed-domain chatbots are specialized in specific areas, offering precise responses. Generative chatbots use advanced {\lm} to create dynamic responses closer to human interactions.

The use of {\llm} allows improvements in the {\nlg} capabilities of chatbots and guides conversations more effectively, especially in the task of defining cohorts in medical research. LLaMa-2, Falcon and Mistral appear to be good options to implement since they are open-source and have the possibility of fine-tuning the model. Fine-tune has the role of optimizing chatbot performance, alongside Prompt engineering and {\rag}. 

In terms of {\ir}, in order to retrieve the most interesting databases according to the user's needs, the {\bm} technique proves to be a good balance between effectiveness and efficiency. {\bm} is a straightforward algorithm that improves upon the traditional {\tfidf} approach. Neural {\ir} systems, like interaction-based models, show good results in the {\ir} tasks, but are complex and the neural networks require substantial computational power. It is a lot simpler to implement than the Neural {\ir} systems, as well as requires fewer computer resources. 


{\ohdsi} provides the software tool, ATLAS, to help researchers conduct observational studies. This tool has a feature to create study cohorts to define groups of people based on the research question. However, the interface revealed not very user-friendly and intuitive because it requires the user to have a good knowledge of its use and important concepts. Therefore, a chatbot that builds a cohort definition for this tool can improve the user experience by making it more intuitive and autonomous.


%  meter mais referencias e citações